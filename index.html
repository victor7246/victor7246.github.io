<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a:any-link {
      /* color: #c9e2fa; */
      color: rgb(0, 14, 204);  
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #0C90FF;
      text-decoration: none;
    }
    
    body {
      background-size: auto;
      background-repeat:no-repeat;
      background-position: center top; 
      margin-top: 15px;
    }

    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      color: rgb(61, 61, 61);
    }

    pmid {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      color: rgb(61, 61, 61);
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      color: black;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
      color: black;
    }

    midheading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      color: black;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700;
      color: black;
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      color: black;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    .bottom-padding {
     margin-bottom: 20px;
    }

    .top-padding {
     margin-top: 20px;
    }

    li{
    margin: 10px 0;
    }
  </style>

  <title>Ayan Sengupta</title>
  
  <meta name="author" content="Ayan Sengupta">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="static/images/github.png">
</head>

<body>
  <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ayan Sengupta</name>
              </p>
              <p>I am a fourth year PhD. student at <a href="https://home.iitd.ac.in//">IIT Delhi</a>, where I work in the <a href="https://www.lcs2.in//">Laboratory for Computational Social Systems (LCS2)</a> lead by Dr. Tanmoy Chakraborty. My primary area of research includes understanding generalization capabilities of small and efficient language models. I lead the <a href="https://parmanu.lcs2.in/"> Parmanu (Micron) group </a> within LCS2, which explores efficient architectures, fine‑tuning, inference, model compression and scaling laws to make large language models accessible on commodity hardware.</p>
              <p> 
              I work as a Principal Data Scientist at Optum, the intelligence arm of UnitedHealth Group, where I lead AI‑first solutions for the Home & Community business, building generative and predictive models that improve patient care and operations. Over the past eight years I’ve progressed from Data Scientist through Senior and Lead roles, guiding cross‑functional teams and deploying scalable predictive modelling and AI/ML projects. As a Master Inventor with seven granted U.S. patents, I serve on Optum’s Patent Review Board and mentor early‑stage inventors. The platforms I’ve built deliver more than $20 million in annual value, highlighting the tangible impact of data science in healthcare.  
              </p>
              <p>
                I completed my Masters in Business Analytics and Data Science (<a href="https://www.isical.ac.in/~pgdba/">PGDBA</a>) from <a href="https://www.iimcal.ac.in/">Indian Institute of Management, Calcutta </a>. Prior to that, I did my Masters of Science in Mathematics and Computing from <a href="http://www.iitg.ac.in/">Indian Institute of Technology, Guwahati </a> in 2015 and Bachelor of Science in Mathematics and Computer Science from <a href="https://www.cmi.ac.in/">Chennai Mathematical Institute </a> in 2013.

              </p>
              <p style="text-align:center">
                <a href="mailto:ayan.sengupta007@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/victor7246">GitHub</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ayans2017/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://www.kaggle.com/datamafia7">Kaggle</a> &nbsp/&nbsp
                <a href="https://www.topcoder.com/members/datamafia">Topcoder</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=90EGfboAAAAJ&hl=en">Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="static/images/picture.jpeg"><img style="width:90%;max-width:90%" alt="profile photo" src="static/images/picture.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="right" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse; margin-top:30px">
          <tbody><tr style="border-bottom: 2px solid rgb(192, 192, 192)">
          <td width="100%" valign="top" align="middle">
            <!-- Updated navigation: use local anchors and add links for experience, research and competitions -->
            <a href="#news"><b>News</b></a> &#8195;<span>·</span>&#8195;
            <a href="#experience"><b>Experience</b></a> &#8195;<span>·</span>&#8195;
            <a href="#research"><b>Research Portfolio</b></a> &#8195;<span>·</span>&#8195;
            <a href="#education"><b>Education</b></a> &#8195;<span>·</span>&#8195;
            <a href="#publications"><b>Selected Publications</b></a> &#8195;<span>·</span>&#8195;
            <a href="#patents"><b>Granted Patents</b></a> &#8195;<span>·</span>&#8195;
            <a href="#competitions"><b>Competitions & OSS</b></a> &#8195;<span>·</span>&#8195;
            <a href="#extra"><b>Extra</b></a> &#8195;<span>·</span>
            </td>
          
          
          </tr>
          </tbody></table>

        <table id="news" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:30px">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading><b>Recent Highlights  </b></heading><img src="static/images/highlights.gif" width='70px'>
              <ul>
                <li><strong>[Dec 2025]</strong> Launch of our Parmanu (Micron) project page - https://parmanu.lcs2.in/</li>
                <li><strong>[Dec 2025]</strong> Our team attended NeurIPS 2025 at San Diago for presenting our work on <em>value-based KV compression</em>.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <!--
          The following custom sections provide a high‑level overview of
          professional experience, research portfolio and competitions/open
          source work.  These sections were added to highlight the
          user’s career achievements and research directions.  Each
          section has its own anchor so the navigation bar can link
          directly to it.  The content summarises key points from
          the user’s résumé and publications without replacing the
          detailed lists that follow later in the page.
        -->

        <!-- Overview section summarising the three key areas -->
        <table id="overview" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:30px">
          <tbody>
            <tr>
              <td colspan="3" valign="middle">
                <heading><b>Overview</b></heading>
              </td>
            </tr>
            <tr>
              <td width="33%" valign="top">
                <midheading><b>Professional Experience</b></midheading>
                <p style="color:rgb(110,110,110)">Principal Data Scientist at UnitedHealth Group with 8+ years of experience and numerous innovation awards and patents.<br><a href="#experience">Learn more…</a></p>
              </td>
              <td width="33%" valign="top">
                <midheading><b>Research Portfolio</b></midheading>
                <p style="color:rgb(110,110,110)">Focused on efficient large language models, low‑resource language modelling and persona‑aware generative modelling.<br><a href="#research">Learn more…</a></p>
              </td>
              <td width="33%" valign="top">
                <midheading><b>Competitions & Open Source</b></midheading>
                <p style="color:rgb(110,110,110)">Bronze medalist in multiple Kaggle competitions and creator of an open‑source JST library.<br><a href="#competitions">Learn more…</a></p>
              </td>
            </tr>
          </tbody>
        </table>

        <!-- Professional Experience section -->
        <table id="experience" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:30px">
          <tbody>
            <tr>
              <td width="100%" valign="middle">
                <heading><b>Professional Experience</b></heading>
              </td>
            </tr>
          </tbody>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="" alt="" width="1" height="1"></td>
              <td width="90%" valign="top">
                <p><papertitle>Principal Data Scientist – UnitedHealth Group</papertitle><br>
                <em>Apr 2025 – Present</em><br>
                </p>
                <p style="color:rgb(110, 110, 110)">
                  Lead the design of AI‑first solutions for home and community business of OptumHealth within a 6-member team.  Member of TLCP (top 0.1%) program. Member of the company’s patent review board, mentoring early stage inventors; recognised with the Master Inventor award for contributing to several U.S. patents.
                </p>
              </td>
            </tr>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="" alt="" width="1" height="1"></td>
              <td width="90%" valign="top">
                <p><papertitle>Various Positions – UnitedHealth Group</papertitle><br>
                <em>May 2017 – Mar 2025</em><br>
                </p>
                <p style="color:rgb(110, 110, 110)">
                  Earned Master Inventor recognition and served on the patent review board. Advanced predictive modelling and natural‑language processing projects. Built and deployed data‑driven models for customer analytics.  Honoured with the 2018 Q4 R&R Mastermind award and third place in a company‑wide analytics championship.
                </p>
              </td>
            </tr>
          </tbody>
        </table>

        <!-- Research Portfolio section -->
        <table id="research" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:30px">
          <tbody>
            <tr>
              <td width="100%" valign="middle">
                <heading><b>Research Portfolio</b></heading>
              </td>
            </tr>
          </tbody>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
        <tbody>

          <!-- Image row -->
          <tr>
            <td align="center">
              <img src="static/images/summary.png"
                  alt="Research summary"
                  style="width:150%;max-width:300px;border-style:none">
            </td>
          </tr>

          <!-- Content row -->
          <tr>
            <td valign="top">
              <p><midheading><b>Efficient Large Language Models</b></midheading></p>
              <p style="color:rgb(110,110,110)">
                Our work advocates for a paradigm shift toward downscaling large language models
                to make them sustainable and deployable. We explore efficient architectures,
                fine-tuning, inference, model compression and scaling laws.
              </p>

              <p><midheading><b>Low-resource Language Modelling</b></midheading></p>
              <p style="color:rgb(110,110,110)">
                We design robust representation learning methods for code-mixed and
                low-resource languages, improving sentiment analysis, NER and language ID. We also work on designing computational linguistic framework for understanding the evolution of code-mixed language
              </p>

              <p><midheading><b>Generative Modelling</b></midheading></p>
              <p style="color:rgb(110,110,110)">
                We develop persona-aware and probabilistic generative models for code-mixed text and aspect-based opinion mining.
              </p>

              <br><a href="#publications">Detailed work…</a>
            </td>
          </tr>

        </tbody>
      </table>


        <table id="education" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:20px">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading><b>Education</b></heading>
            </td>
          </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="static/images/iitd.png" alt="PontTuset" width="80" style="border-style: none">
                </td><td width="90%" valign="top">
                    <p><papertitle>PhD, Computer Science</papertitle> 
                      <br>
                      <em>Indian Institute of Technology, Delhi</em>
                      <br> 
                      2023-Present
                      <br>
                      </p><p style="color:rgb(110, 110, 110)">
                        Advisors: <a href="https://www.tanmoychak.com/">Dr. Tanmoy Chakraborty</a>
                        <p style="color:rgb(110, 110, 110)">  
                          Leading Parmanu (Micron) project on efficient large language models within <a href="https://www.lcs2.in//">LCS2 lab</a>.
                      </p>
                </td>
            </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="static/images/iiitd.png" alt="PontTuset" width="80" style="border-style: none">
                </td><td width="90%" valign="top">
                    <p><papertitle>PhD, Computer Science</papertitle> 
                      <br>
                      <em>Indraprastha Institute of Information Technology, Delhi</em>
                      <br> 
                      2021-2022
                      <br>
                      </p><p style="color:rgb(110, 110, 110)">
                        Advisors: Dr. Tanmoy Chakraborty and <a href="https://iiitd.ac.in/shad">Dr. Md. Shad Akhtar</a>
                      </p>
                      <p style="color:rgb(110, 110, 110)">  
                        Courses: Machine Learning, Natural Language Processing, Social Network Analysis, Data Mining, Artificial Intelligence, Bayesian Machine Learning
                    </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="static/images/iim.png" alt="PontTuset" width="80" style="border-style: none">
                </td><td width="90%" valign="top">
                    <p><papertitle>MS, Business Analytics and Data Science</papertitle> 
                      <br>
                      <em>Indian Institute of Management, Calcutta </em>(jointly with <em> Indian Institute of Technology, Kharagpur </em> and <em> Indian Statistical Institute, Kolkata</em>)
                      <br> 
                      2015-2017
                      <br>
                      </p><p style="color:rgb(110, 110, 110)">
                        Courses: Algorithms, Machine Learning, Multivariate Analysis, Complex Networks, Information Retrieval, Econometrics, Statistical Inference   
                    </p>
                </td>
            </tr>
          
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="static/images/iit.png" alt="PontTuset" width="80" style="border-style: none">
                </td><td width="90%" valign="top">
                    <p><papertitle>Masters of Science, Mathematics and Computing</papertitle> 
                      <br>
                      <em>Indian Institute of Technology, Guwahati</em>
                      <br> 
                      2013-2015
                      <br>
                      </p><p style="color:rgb(110, 110, 110)">
                        Advised by <a href="http://www.iitg.ac.in/a.saikia/">Prof. Anupam Saikia</a> for masters thesis on <em> Mathematics of Elliptic Curves and its application in Cryptography</em>
                      </p>
                      </p><p style="color:rgb(110, 110, 110)">
                        Courses: Algorithms, Logic Programming (Prolog, Introduction to AI), Probability Theory, Numerical Analysis, Optimization 
                    </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="10%" align="center"><img src="static/images/cmi.png" alt="PontTuset" width="80" style="border-style: none">
                </td><td width="90%" valign="top">
                    <p><papertitle>Bachelor of Science, Mathematics and Computer Science</papertitle> 
                      <br>
                      <em>Chennai Mathematical Institute</em>
                      <br> 
                      2010-2013
                      <br>
                      </p><p style="color:rgb(110, 110, 110)">
                        Courses: Mathematical Logic, Algorithm Design, Game Theory, Theory of Computation, Programming 
                    </p>
                </td>
            </tr>

          </tbody></table>
        
          <table id="publications" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:20px">
            <tbody><tr>
              <td width="100%" valign="middle">
                <heading><b>Selected Publications</b></heading>
                <font size="1">(* denotes equal contribution)</font>
                <br>
                All publications are available on <a href="https://scholar.google.com/citations?user=90EGfboAAAAJ&hl=en">Google Scholar</a>.
                <br>
              </td>
            </tr>
        </tbody></table>
        
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/curdKV_header.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>Value-Guided KV Compression for LLMs via Approximated CUR Decomposition</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>*, Siddhant Chaudhary*, Tanmoy Chakraborty
                    <br>
                    <em> Conference on Neural Information Processing Systems (NeurIPS) 2025</em>
                    <br>
                    <a href="https://openreview.net/forum?id=klmc4fwPLd">Openreview</a> | <a href="https://github.com/NVIDIA/kvpress/">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output , ensuring that the retained tokens best preserve the model's predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40% at high compression, offering a practical speed-accuracy tradeoff.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/downscaling.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>Position: Enough of Scaling LLMs! Lets Focus on Downscaling</papertitle>
                    
                    <br>
                    Yash Goel*, <strong>Ayan Sengupta</strong>*, Tanmoy Chakraborty
                    <br>
                    <em> International Conference on Machine Learning (ICML) 2025</em>
                    <br>
                    <a href="https://openreview.net/forum?id=CYJlJgEzZs">Openreview</a> | <a href="https://github.com/LCS2-IIITD/Downscaling">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.
                  </p>
                </td>
            </tr>
            
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/kd_analysis.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>On the Generalization vs Fidelity Paradox in Knowledge Distillation</papertitle>
                    
                    <br>
                    Suhas Kamasetty Ramesh*, <strong> Ayan Sengupta*</strong>, Tanmoy Chakraborty
                    <br>
                    <em> Findings of the Association for Computational Linguistics (ACL) 2025 </em>
                    <br>
                    <a href="https://aclanthology.org/2025.findings-acl.923/">Paper</a> | <a href="https://pypi.org/project/llm-distil/">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to 10%, with a peak task specific gain of 22%, while providing only marginal benefits (∼ 1.3%) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students’ performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/id3.jpg" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>Step-by-step unmasking for parameter-efficient fine-tuning of large language models</papertitle>
                    
                    <br>
                    Aradhye Agarwal*, Suhas Kamasetty Ramesh*, <strong> Ayan Sengupta *</strong>, Tanmoy Chakraborty
                    <br>
                    <em> Transactions of the Association for Computational Linguistics (TACL) 2025</em>
                    <br>
                    <a href="https://direct.mit.edu/tacl/article/doi/10.1162/TACL.a.59/134534">Paper</a> | <a href="https://pypi.org/project/selective-optimizers/">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. Selective-PEFT, a class of parameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters. Although parameter-efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection. Traditional selective-PEFT techniques use a fixed set of parameters selected using different importance heuristics, failing to capture parameter importance dynamically and often leading to suboptimal performance. We introduce ID3, a novel selective-PEFT method that calculates parameter importance continually, and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection. Our empirical study on 16 tasks spanning natural language understanding, mathematical reasoning, and summarization demonstrates the effectiveness of our method compared to fixed-masking selective-PEFT techniques. We analytically show that ID3 reduces the number of gradient updates by a factor of two, enhancing computational efficiency. Since ID3 is robust to random initialization of neurons and operates directly on the optimization process, it is highly flexible and can be integrated with existing additive and reparameterization-based PEFT techniques such as Adapters and LoRA, respectively
                  </p>
                </td>
            </tr>
            
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/monteclora.jpg" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>Robust and efficient fine-tuning of llms with bayesian reparameterization of low-rank adaptation</papertitle>
                    
                    <br>
                    <strong> Ayan Sengupta* </strong>, Vaibhav Seth*, Arinjay Pathak*, Aastha Verma, Natraj Raman, Sriram Gopalakrishnan, Niladri Chatterjee, Tanmoy Chakraborty
                    <br>
                    <em> Transactions on Machine Learning Research (TMLR) 2025</em>
                    <br>
                    <a href="https://openreview.net/forum?id=2HFmicB8kh">Openreview</a> | <a href="https://github.com/parmanu-lcs2/peft/tree/main/examples/monteclora_finetuning">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Large Language Models (LLMs) are highly resource-intensive to fine-tune due to their enormous size. While low-rank adaptation is a prominent parameter-efficient fine-tuning approach, it suffers from sensitivity to hyperparameter choices, leading to instability in model performance on fine-tuning downstream tasks. This paper highlights the importance of effective parameterization in low-rank fine-tuning to reduce estimator variance and enhance the stability of final model outputs. We propose MonteCLoRA, an efficient fine-tuning technique that employs Monte Carlo estimation to learn an unbiased posterior estimation of low-rank parameters with low expected variance, stabilizing fine-tuned LLMs with only O(r) additional parameters, for a given rank r. MonteCLoRA shows 0.5% and 1.6% improvements in accuracy and robustness over unregularized low-rank adaptation method on natural language understanding tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with pre-trained LLaMA-1-7B and LLaMA-3.2-3B-Instruct, MonteCLoRA demonstrates robust performance with 50% and 62% lower spreads respectively than the contemporary efficient fine-tuning methods. The theoretical and empirical results presented in the paper underscore how parameterization and hyperpriors balance exploration-exploitation in the low-rank parametric space, therefore leading to more optimal and robust parameter estimation during efficient fine-tuning.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/prunenet.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>*, Siddhant Chaudhary*, Tanmoy Chakraborty
                    <br>
                    <em>International Conference on Learning Representations (ICLR) 2025</em>
                    <br>
                    <a href="https://openreview.net/forum?id=5RZoYIT3u6">Openreview</a> | <a href="https://pypi.org/project/efficient-pruners/">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose PruneNet, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its zero-shot performance with a 30% compression ratio, outperforming existing methods that retain only 75% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80% performance of the original model, proving itself a superior alternative to conventional structured compression techniques.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/mpdistil.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>, Shantanu Dixit, Md Shad Akhtar, Tanmoy Chakraborty
                    <br>
                    <em>International Conference on Learning Representations (ICLR) 2024</em>
                    <br>
                    <a href="https://openreview.net/forum?id=Ixi4j6LtdX">Openreview</a> | <a href="https://pypi.org/project/mpdistil/">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Knowledge distillation (KD) is a technique used to transfer knowledge from a larger “teacher” model into a smaller “student” model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the finetuning of teacher models should be aware of the student’s need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both collaboration and competition during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to 20 conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model– e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on f ive out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just 4.6% on SuperGLUE. Wefurther demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/paradox.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>Persona-aware Generative Model for Code-mixed Language</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>, Md Shad Akhtar, Tanmoy Chakraborty
                    <br>
                    <em>Transactions on Machine Learning Research (TMLR) 2024</em>
                    <br>
                    <a href="https://openreview.net/forum?id=fzP4qIiVIh">Openreview</a> | <a href="https://github.com/victor7246/PARADOX">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user’s preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models tend to ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose PARADOX, a persona-aware generative model for code-mixed text generation, which is a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user’s persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARADOX, we propose four new metrics– CM BLEU, CM Rouge-1, CM Rouge-L and CM KS. On average, PARADOX achieves 1.6% better CM BLEU, 57% better perplexity and 32% better semantic coherence than the non-persona-based counterparts.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/transject.png" alt="PontTuset" width="190" style="border-style: none">
                </td><td width="75%" valign="top">
                  <p>
                    <papertitle>Manifold-Preserving Transformers are Effective for Short-Long Range Encoding</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>, Md Shad Akhtar, Tanmoy Chakraborty
                    <br>
                    <em>Findings of the Association for Computational Linguistics (EMNLP) 2023</em>
                    <br>
                    <a href="https://arxiv.org/abs/2310.14206">Paper</a> | <a href="https://pypi.org/project/transject/">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens. We propose a simple alternative to dot-product attention to ensure Lipschitz continuity. This allows TransJect to learn injective mappings to transform token representations to different manifolds with similar topology and preserve Euclidean distance between every pair of tokens in subsequent layers. Evaluations across multiple benchmark short- and long-sequence classification tasks show maximum improvements of 6.8% and 5.9%, respectively, over the variants of Transformers. Additionally, TransJect displays 79% better performance than Transformer on the language modeling task. We further highlight the shortcomings of multi-head self-attention from the statistical physics viewpoint. Although multi-head self-attention was incepted to learn different abstraction levels within the networks, our empirical analyses suggest that different attention heads learn randomly and unorderly. In contrast, TransJect adapts a mixture of experts for regularization; these experts are more orderly and balanced and learn different sparse representations from the input sequences. TransJect exhibits very low entropy and can be efficiently scaled to larger depths.
                  </p>
                </td>
            </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/hate.jpeg" alt="PontTuset" width="170" style="border-style: none">
                <p>
                  <font size="1" px="">Image Credits:</font><a href="https://www.dailyprincetonian.com/multimedia/hate-speech-1"><font size="1" px=""> Link</font></a>
                </p>
                </td><td width="75%" valign="top">
                  <p>
                    <!-- <a href="http://aclweb.org/anthology/K18-1034" id="conll2018"> -->
                      <papertitle>Does aggression lead to hate? Detecting and reasoning offensive traits in hinglish code-mixed texts</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>, Sourabh Kumar Bhattacharjee, Md Shad Akhtar, Tanmoy Chakraborty
                    <br>
                    <em>Neurocomputing | Elsevier</em>
                    <br>
                    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221017306">Paper</a> | <a href="https://github.com/LCS2-IIITD/Hinglish_offense_detection-Neurocomputing2021">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Aggression is a prominent trait of human beings that can affect social harmony in a negative way. The hate mongers misuse the freedom of speech in social media platforms to flood with their venomous comments in many forms. Identifying different traits of online offense is thus inevitable and the need of the hour. Existing studies usually handle one or two offense traits at a time, mainly due to the lack of a combined annotated dataset and a scientific study that provides insights into the relationship among the traits. In this paper, we study the relationship among five offense traits – aggression, hate, sarcasm, humor, and stance in Hinglish (Hindi-English) social media code-mixed texts. We employ various state-of-the-art deep learning systems at different morphological granularities for the classification across five offense traits. Our evaluation of the unified framework suggests  performance across all major traits. Furthermore, we propose a novel notion of causal importance score to quantify the effect of different abusive keywords and the overall context on the offensiveness of the texts.
                  </p>
                </td>
            </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/hit.png" alt="PontTuset" width="170" style="border-style: none">
                <p>
                  <font size="1" px="">Image Credits:</font><a href="https://owlcation.com/humanities/Code-Switching-Definition-Types-and-Examples-of-Code-Switching"><font size="1" px=""> Link</font></a>
                </p>
                </td><td width="75%" valign="top">
                  <p>
                    <!-- <a href="http://aclweb.org/anthology/K18-1034" id="conll2018"> -->
                      <papertitle>HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>, Sourabh Kumar Bhattacharjee, Tanmoy Chakraborty, Md Shad Akhtar
                    <br>
                    <em>Findings of the Association for Computational Linguistics (ACL-IJCNLP) 2021</em>
                    <br>
                    <a href="https://aclanthology.org/2021.findings-acl.407/">Paper</a> | <a href="https://github.com/LCS2-IIITD/Code-mixed-classification">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Understanding linguistics and morphology of resource-scarce code-mixed texts remains a key challenge in text processing. Although word embedding comes in handy to support downstream tasks for low-resource languages, there are plenty of scopes in improving the quality of language representation particularly for code-mixed languages. In this paper, we propose HIT, a robust representation learning method for code-mixed texts. HIT is a hierarchical transformer-based framework that captures the semantic relationship among words and hierarchically learns the sentence-level semantics using a fused attention mechanism. HIT incorporates two attention modules, a multi-headed self-attention and an outer product attention module, and computes their weighted sum to obtain the attention weights. Our evaluation of HIT on one European (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and Malayalam) languages across four NLP tasks on eleven datasets suggests significant performance improvement against various state-of-the-art systems. We further show the adaptability of learned representation across tasks in a transfer learning setup (with and without fine-tuning).
                  </p>
                </td>
            </tr>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
              <td width="25%" align="center"><img src="static/images/eljst.png" alt="PontTuset" width="170" style="border-style: none">
                <p>
                  <font size="1" px="">Image Credits:</font><a href="https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05"><font size="1" px=""> Link</font></a>
                </p>
                </td><td width="75%" valign="top">
                  <p>
                    <!-- <a href="http://aclweb.org/anthology/K18-1034" id="conll2018"> -->
                      <papertitle>An Embedding-based Joint Sentiment-Topic Model for Short Texts</papertitle>
                    
                    <br>
                    <strong>Ayan Sengupta</strong>*, William Scott Paka*, Suman Roy, Gaurav Ranjan, Tanmoy Chakraborty
                    <br>
                    <em>Vol. 15 (2021): Fifteenth International AAAI Conference on Web and Social Media</em>
                    <br>
                    <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/18090">Paper</a> | <a href="https://github.com/victor7246/ELJST-Paper">Code</a>
                    <br>
                  <p style="color:rgb(156, 156, 156)">
                    Short text is a popular avenue of sharing feedback, opinions and reviews on social media, e-commerce platforms, etc. Many companies need to extract meaningful information (which may include thematic content as well as semantic polarity) out of such short texts to understand users’ behaviour. However, obtaining high quality sentiment-associated and human interpretable themes still remains a challenge for short texts. In this paper we develop ELJST, an embedding enhanced generative joint sentiment-topic model that can discover more coherent and diverse topics from short texts. It uses Markov Random Field Regularizer that can be seen as generalisation of skip-gram based models. Further, it can leverage higher order semantic information appearing in word embedding, such as self-attention weights in graphical models. Our results show an average improvement of 10% in topic coherence and 5% in topic diversification over baselines. Finally, ELJST helps understand users' behaviour at more granular levels which can be explained. All these can bring significant values to service and healthcare industries often dealing with customers.
                  </p>
                </td>
            </tr>  
          </tbody></table>

      <table id="patents" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:30px">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading><b>Granted Patents  </b></heading>
              <ul>
                <li>US12112132: NATURAL LANGUAGE PROCESSING MACHINE LEARNING FRAMEWORKS TRAINED USING MULTI-TASK TRAINING ROUTINES</li>
                <li>US11210818: SUPERVISED AND UNSUPERVISED MACHINE LEARNING TECHNIQUES FOR COMMUNICATION SUMMARIZATION</li>
                <li>US12229512: SIGNIFICANCE-BASED PREDICTION FROM UNSTRUCTURED TEXT</li>
                <li>US11698934: GRAPH EMBEDDING BASED PARAGRAPH VECTOR MACHINE LEARNING MODELS</li>
                <li>US11494565: NATURAL LANGUAGE PROCESSING TECHNIQUES USING JOINT SENTIMENT-TOPIC MODELING</li>
                <li>US12008321: NATURAL LANGUAGE PROCESSING TECHNIQUES FOR SEQUENTIAL TOPIC MODELING</li>
                <li>US11068666: NATURAL LANGUAGE PROCESSING USING JOINT SENTIMENT-TOPIC MODELING</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

      <table id="competitions" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:20px">
              <tbody><tr>
                <td width="100%" valign="middle">
                  <heading><b>Competitions & Open Source</b></heading>
                </td>
              </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
                  <td width="10%" align="center"><img src="static/images/Kaggle2023.png" alt="PontTuset" width="170" style="border-style: none">
                    </td><td width="90%" valign="top">
                        <p><papertitle>2023 Kaggle AI Report</papertitle> 
                          <br>
                          <a href="https://www.kaggle.com/code/datamafia7/a-text-odyssey-the-past-present-and-future-of-nlp">Code</a>
                          <br>
                          </p><p style="color:rgb(110, 110, 110)">
                           In this challenge, the participants were asked to write an essay on one of the following seven chosen topics from the field of AI, with a prompt to describe what the community has learned over the past 2 years of working and experimenting with.
                           <br>
                        </p>
                    </td>
                </tr>

             <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
                  <td width="10%" align="center"><img src="static/images/jigsaw.png" alt="PontTuset" width="170" style="border-style: none">
                    </td><td width="90%" valign="top">
                        <p><papertitle>Kaggle: Jigsaw Multilingual Toxic Comment Classification Challenge 2020</papertitle> 
                          <br>
                          <a href="https://www.kaggle.com/datamafia7/ensemble-with-features">Code</a> | <a href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification">Link</a>
                          <br>
                          </p><p style="color:rgb(110, 110, 110)">
                           In this challenge, the objective is to classify whether a comment is toxic (or, abusive). The dataset contains multi-lingual comments from different Wikipedia talk pages. We experimented with various multilingual transformer models, Universal Sentence Encoder (USE) and their ensembles. 
                           <br>
                           <br>
                           Final private leaderboard rank achieved 156 (Bronze medal)
                        </p>
                    </td>
                </tr>

            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
                  <td width="10%" align="center"><img src="static/images/googleqa.png" alt="PontTuset" width="170" style="border-style: none">
                    </td><td width="90%" valign="top">
                        <p><papertitle>Kaggle: Google QUEST Q&A Labeling Challenge 2020</papertitle> 
                          <br>
                          <a href="https://github.com/victor7246/Competition-Solutions/tree/master/Kaggle_Google_QA">Code</a> | <a href="https://www.kaggle.com/c/google-quest-challenge">Link</a>
                          <br>
                          </p><p style="color:rgb(110, 110, 110)">
                           In this challenge, the objective is to predict different subjective aspects of question-answering gathered from different StackExchange properties. In this competition, we explored BERT model and hand picked feature engineering to develop a robust model that can predict the subjectivity metrics accurately. 
                           <br>
                           <br>
                           Final private leaderboard rank achieved 116 (Bronze medal)
                        </p>
                    </td>
                </tr>

                <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
                  <td width="10%" align="center"><img src="static/images/gammalog.jpeg" alt="PontTuset" width="170" style="border-style: none">
                    <p>
                      <font size="1" px="">Image Credits:</font><a href="http://www.sepmstrata.org/page.aspx?pageid=168"><font size="1" px=""> Link</font></a>
                    </p>
                    </td><td width="90%" valign="top">
                        <p><papertitle>CrowdANALYTIX: Gamma Log Facies Type Prediction Challenge 2019</papertitle> 
                          <br>
                          <a href="https://github.com/victor7246/Competition-Solutions/tree/master/Gamma%20ray%20Classification">Code</a> | <a href="https://www.crowdanalytix.com/contests/gamma-log-facies-type-prediction">Link</a>
                          <br>
                          </p><p style="color:rgb(110, 110, 110)">
                           Given an array of GR (Gamma-Ray) values, accurately predict the log facies type corresponding to each value. Solution includes stacking of several seq2seq models with attention. Achieved overall 96.8% accuracy.
                           <br>
                           <br>
                           Final private leaderboard rank achieved 26 (out of 350 teams)
                        </p>
                    </td>
                </tr>

                <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
                  <td width="10%" align="center"><img src="static/images/rooftop.jpeg" alt="PontTuset" width="170" style="border-style: none">
                    <p>
                      <font size="1" px="">Image Credits:</font><a href="https://roofonline.com/satellite-images"><font size="1" px=""> Link</font></a>
                    </p>
                    </td><td width="90%" valign="top">
                        <p><papertitle>Data Science Game 2016: Online selection</papertitle> 
                          <br>
                          <a href="http://www.majumderb.com/">Bodhisattwa Prasad Majumder</a>, Robin Singh, <strong>Ayan Sengupta</strong>, Jayanta Mandi
                          <br>
                          <a href="https://github.com/victor7246/DataScienceGame2016">Code</a> | <a href="https://www.kaggle.com/c/data-science-game-2016-online-selection/">Link1</a> | <a href="https://datasciencegame.com/last-edition/">Link2</a>
                          <br>
                          </p><p style="color:rgb(110, 110, 110)">
                           The challenge was to classify orientation of building roofs using satellite images of roof tops. We used different image augmentation techniques along with VGG network and achieved 82% accuracy on test dataset.
                           <br>
                           <br>
                           Final private leaderboard rank achieved 22 (out of 110 teams) and got selected for Finals of Data Science Game 2016
                        </p>
                    </td>
                </tr>

              </tbody></table>

       <table id="development" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:20px">
              <tbody><tr>
                <td width="100%" valign="middle">
                  <heading><b>Development Projects</b></heading>
                </td>
              </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
          <tbody>
            <tr style="border-bottom: 1px solid rgb(192, 192, 192);">
                  <td width="10%" align="center"><img src="static/images/jts.png" alt="PontTuset" width="170" style="border-style: none">
                    </td><td width="90%" valign="top">
                        <p><papertitle><em>jointtsmodel</em> - Python package for Probabilistic Joint Topic-Sentiment Models</papertitle> 
                          <br>
                          <a href="https://github.com/victor7246/jointtsmodel">Code</a>
                          <br>
                          </p><p style="color:rgb(110, 110, 110)">
                            Joint topic-sentiment models aka. aspect rating models can extract thematic representation from texts at a granular level. In the areas of customer relationship management, it is utmost important to understand the pain point areas of customers from their feedbacks, complaints data and other data sources. By leveraging both sentiment as well as, thematic information, JST based models can understand the intent at overall level as well as, at theme level, which makes it perfect for analyzing voice of the customers. 
                            <br>
                            <br>
                            This package contains implementation of various probabilistic generative JST models from the literature.
                        </p>
                    </td>
                </tr>
            </tbody></table>

        <table id="extra" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:30px">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading><b>Extra</b></heading>
              <ul>
                <li><strong>[2019]</strong> Passed Grade 1 in Rock & Pop Bass, awarded by Trinity College London 🤘</li>
                <li><strong>[2019]</strong> Performed at Rockathon organized by Noida School of Rock 🤘🤘</li>
                <li><strong>[2016]</strong> Passed A1 in French by CdA Global Language Centre</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td>
              <br>
              <p>
                <a href="https://info.flagcounter.com/gPZB"><img src="https://s04.flagcounter.com/count2/gPZB/bg_FFFFFF/txt_000000/border_CCCCCC/columns_4/maxflags_20/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
              </p>
              <p align="right">
                <font size="2">
                  Thanks to <a href="https://jonbarron.info/">Jon Barron</a> and <a href="http://www.majumderb.com/">Bodhisattwa P. Majumder</a> for this nice template.
                  </font>
              </p>
            </td>
          </tr>
        </tbody></table>
        </td>
    </tr>
  </tbody></table>

  </td>
    </tr>
  </table>
</body>

</html>
